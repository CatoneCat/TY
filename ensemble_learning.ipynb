{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集成学习(Ensemble Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假定在项目中训练好了一些分类器  \n",
    "可能包括: Logistic Regression, SVM, Random Forest, K-Nearest Neighbors,etc.  \n",
    "选取预测结果中最多的那一分类,作为集成学习的输出分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只有当预测器间相互独立时,集成方法才能获得最好的效果  \n",
    "一种获得多样化预测器的方法是使用完全不同的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g. 使用三种不同的算法集成Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "rnd_clf = RandomForestClassifier(random_state=42)\n",
    "svm_clf = SVC(random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看每一种分类器的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soft voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果所有的分类器都能预测出分类的概率,则可以计算所有分类器预测概率的平均值,选取概率最大的为输出分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常,soft voting可以取得比hard voting更好的效果  \n",
    "在使用中,只需要改变VotingClassifier()中的参数, voting='soft'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一种获得多样化分类器的方法是使用完全不同的训练算法  \n",
    "另一种方法是, 在训练集的不同随机子集中使用相同的训练算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在选取训练子集的时候:  \n",
    "1.如果抽样时将样本放回(with replacement) , 则为bagging  \n",
    "2.如果抽样时不将样本放回(without replacement) , 则为pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在多个预测器中,bagging和pasting都允许同一个训练实例被抽样多次  \n",
    "但在同一个预测器中,只有bagging允许同一个训练实例被抽样多次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终的输出结果通常由所有预测器的预测结果聚合而成  \n",
    "聚合函数(aggregation function):  \n",
    "对于分类问题,通常选取频率最高的预测分类  \n",
    "对于回归问题,通常选取预测回归的平均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有预测器都是分开平行训练的,因此,可以通过完全不同的CPU核心甚至不同的服务器来进行运算  \n",
    "类似地,预测也可以通过平行方法来进行预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn通过BaggingClassifier类,同时提供了bagging和pasting方法\n",
    "对于回归问题,使用BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators=500 : 表示训练500个决策树分类器  \n",
    "max_samples=100 : 每个分类器随机抽样100个实例,如果使用0.1~1.0,则按样本比例进行抽样   \n",
    "bootstrap : True表示使用bagging, False表示使用pasting    \n",
    "n_jobs : 表示需要使用的CPU核心数, -1表示使用所有可用的核心数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果算法拥有predict_proba()方法,BaggingClassifier将自动使用soft voting(概率预测)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "集成方法通常会得到一个相似的bias(欠拟合误差),但会得到一个更小的variance(过拟合误差)  \n",
    "bagging在每个子集上的多样性上更强一些, 因此,bagging的bias会高于pasting,但其variance则会减小(更不容易过拟合)  \n",
    "通常,bagging的表现会由于pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag(oob) Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用bagging的情况下,对于任一给定预测器, 一个实例可能会被抽样多次,而其他可能根本不会被抽样  \n",
    "默认下,BaggingClassifier有放回的抽样m个实例(m为训练样本的数据总量)  \n",
    "根据统计学, 这意味着对于每一个分类器,平均抽取训练样本总量中63%的样本  \n",
    "剩余的37%未被抽取的样本成为out-of-bag(oob)  注: 并非所有的分类器的oob都是37%,所有分类器oob的平均值为37%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为一个预测器在训练的过程中从未见过oob数据,所以可以直接用oob数据进行验证,而无需额外分离出验证数据或使用交叉验证  \n",
    "可以验证每一个预测器在其自身oob数据上的结果的平均值,作为集成的验证结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Scikit-Learn中,可以设置oob_score=True,在训练后自动进行oob评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    bootstrap=True, n_jobs=-1, oob_score=True, random_state=40)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该属性返回每一个训练实例的决策函数,如果算法可以评估概率,则返回的是每一个实例分类的所有概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Patches and Random Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BaggingClassifier类也支持从特征中进行抽样  \n",
    "max_features : 设置最大的抽样特征数  \n",
    "bootstrap_features : 设置有放回特征抽样 or 无放回特征抽样  \n",
    "每一个预测器将在一个拥有随机特征的子集上进行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在一个高维的输入数据(e.g.图像)上,这一方法尤为有用  \n",
    "Random Patches \n",
    "在实例和特征上都进行抽样的方法称为Random Patches  \n",
    "Random Subspaces\n",
    "保持所有训练实例(i.e.,bootstrap=False & max_samples=1.0),  \n",
    "但对特征进行抽样(i.e.,bootstrap_feature & max_features <1.0 ) 的方法称为Random Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征抽样会使得预测器更为多样化,在提升一点bias的情况下,获得更低的variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林(Random Forests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于回归问题,使用RandomForestRegressor  \n",
    "RandomForestClassifier拥有所有决策树分类器的超参数, 也拥有BaggingClassifier的超参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林算法引入了更多随机特性  \n",
    "在训练的过程中也进行了特征的抽样,从一个随机的特征子集中选取特征进行节点的分类  \n",
    "以下BaggingClassifier大致等同于上述的RandomForestClassifier代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=42),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra-Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练一个随机森林算法时:\n",
    "一种方法是,在每一个节点,只选取一个随机的特征子集进行分类(split)  \n",
    "第二种方式是, 在第一种方法的前提下, 在每一个节点使用随机的特征阈值来进行分类(split) ,而不是找到最优的特征阈值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种极端的随机树方法称为: Extremely Randomized Trees ensemble  \n",
    "该方法会产生一个更大的bias和一个更小的variance,同时,该方法的训练速度也更快(因为无需找到最优化的特征阈值)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn 中的API为ExtraTreesClassifier和ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常,只有在实际验证后,才能得知Random Forests 和Extra Trees之间孰优孰劣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在一个决策树中,重要的特征更可能出现在决策树的顶部, 不重要的特征更可能出现在决策树的底部  \n",
    "在理论上,通过计算一个特征在整个森林中的平均深度来评估该特征的重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnd_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林能够非常方便地得出哪些特征更为重要,尤其是在特征选择中的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting (Hypothesis boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting方法是一种用来提高弱分类算法准确度的方法,这种方法通过构造一个预测函数序列,然后以一定的方式将他们组合成一个预测函数。  \n",
    "预测函数序列: 每一个函数都试图修正前一个预测函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.第一个分类器使用整体训练集来进行训练和预测    \n",
    "2.提升训练数据中被错误分类的实例权重  \n",
    "3.第二个分类器用提升过权重的数据来进行训练,并在原训练数据上进行预测  \n",
    "4.依次循环"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost有一个很重要的缺点  \n",
    "因为它是一个序列学习技术,因此无法使用平行运算方法  \n",
    "每一个预测函数都必须在前一个预测函数训练好,并评估好以后才能接下去运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在AdaBoost算法的开始,每一个实例的权重都为$\\frac{1}{m}$  \n",
    "第一个预测函数训练好后,它的加权误差率$r_1$通过训练集来进行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "第j个预测函数的加权误差率(weighted error rate)  \n",
    "$\\large\n",
    "r_j = \\dfrac{\\displaystyle \\sum\\limits_{\\textstyle {i=1 \\atop \\hat{y}_j^{(i)} \\ne y^{(i)}}}^{m}{w^{(i)}}}{\\displaystyle \\sum\\limits_{i=1}^{m}{w^{(i)}}} \\quad\n",
    "\\text{where }\\hat{y}_j^{(i)}\\text{ is the }j^{\\text{th}}\\text{ predictor's prediction for the }i^{\\text{th}}\\text{ instance.}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测函数的权重:  \n",
    "$\\large\n",
    "\\begin{split}\n",
    "\\alpha_j = \\eta \\log{\\dfrac{1 - r_j}{r_j}}\n",
    "\\end{split}\n",
    "$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
